{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf98f703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\users\\chandu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.8.2.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\users\\chandu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wordcloud) (1.21.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\chandu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wordcloud) (3.4.3)\n",
      "Requirement already satisfied: pillow in c:\\users\\chandu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from wordcloud) (8.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\chandu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\chandu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\chandu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->wordcloud) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\chandu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\chandu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "007e094e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\CHANDU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import wordcloud\n",
    "\n",
    "#to data preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#NLP tools\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#train split and fit models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "834c3d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 384.0 kB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in c:\\users\\chandu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "     -------------------------------------- 78.5/78.5 kB 483.9 kB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\users\\chandu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2022.10.31-cp39-cp39-win_amd64.whl (267 kB)\n",
      "     ------------------------------------ 267.8/267.8 kB 279.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\chandu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm->nltk) (0.4.4)\n",
      "Installing collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.8.1 regex-2022.10.31 tqdm-4.64.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53bd9de",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Importing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fc5fe4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24778</th>\n",
       "      <td>25291</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24779</th>\n",
       "      <td>25292</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>you've gone and broke the wrong heart baby, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24780</th>\n",
       "      <td>25294</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24781</th>\n",
       "      <td>25295</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>youu got wild bitches tellin you lies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24782</th>\n",
       "      <td>25296</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24783 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0               0      3            0                   0        3      2   \n",
       "1               1      3            0                   3        0      1   \n",
       "2               2      3            0                   3        0      1   \n",
       "3               3      3            0                   2        1      1   \n",
       "4               4      6            0                   6        0      1   \n",
       "...           ...    ...          ...                 ...      ...    ...   \n",
       "24778       25291      3            0                   2        1      1   \n",
       "24779       25292      3            0                   1        2      2   \n",
       "24780       25294      3            0                   3        0      1   \n",
       "24781       25295      6            0                   6        0      1   \n",
       "24782       25296      3            0                   0        3      2   \n",
       "\n",
       "                                                   tweet  \n",
       "0      !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
       "...                                                  ...  \n",
       "24778  you's a muthaf***in lie &#8220;@LifeAsKing: @2...  \n",
       "24779  you've gone and broke the wrong heart baby, an...  \n",
       "24780  young buck wanna eat!!.. dat nigguh like I ain...  \n",
       "24781              youu got wild bitches tellin you lies  \n",
       "24782  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...  \n",
       "\n",
       "[24783 rows x 7 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"C:\\\\Users\\\\CHANDU\\\\Downloads\\\\labeled_data.csv\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6039add0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0           0      3            0                   0        3      2   \n",
       "1           1      3            0                   3        0      1   \n",
       "2           2      3            0                   3        0      1   \n",
       "3           3      3            0                   2        1      1   \n",
       "4           4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf3e7928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24783 entries, 0 to 24782\n",
      "Data columns (total 7 columns):\n",
      " #   Column              Non-Null Count  Dtype \n",
      "---  ------              --------------  ----- \n",
      " 0   Unnamed: 0          24783 non-null  int64 \n",
      " 1   count               24783 non-null  int64 \n",
      " 2   hate_speech         24783 non-null  int64 \n",
      " 3   offensive_language  24783 non-null  int64 \n",
      " 4   neither             24783 non-null  int64 \n",
      " 5   class               24783 non-null  int64 \n",
      " 6   tweet               24783 non-null  object\n",
      "dtypes: int64(6), object(1)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d3477ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <td>24783.0</td>\n",
       "      <td>12681.192027</td>\n",
       "      <td>7299.553863</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6372.5</td>\n",
       "      <td>12703.0</td>\n",
       "      <td>18995.5</td>\n",
       "      <td>25296.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>24783.0</td>\n",
       "      <td>3.243473</td>\n",
       "      <td>0.883060</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hate_speech</th>\n",
       "      <td>24783.0</td>\n",
       "      <td>0.280515</td>\n",
       "      <td>0.631851</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>offensive_language</th>\n",
       "      <td>24783.0</td>\n",
       "      <td>2.413711</td>\n",
       "      <td>1.399459</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neither</th>\n",
       "      <td>24783.0</td>\n",
       "      <td>0.549247</td>\n",
       "      <td>1.113299</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <td>24783.0</td>\n",
       "      <td>1.110277</td>\n",
       "      <td>0.462089</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      count          mean          std  min     25%      50%  \\\n",
       "Unnamed: 0          24783.0  12681.192027  7299.553863  0.0  6372.5  12703.0   \n",
       "count               24783.0      3.243473     0.883060  3.0     3.0      3.0   \n",
       "hate_speech         24783.0      0.280515     0.631851  0.0     0.0      0.0   \n",
       "offensive_language  24783.0      2.413711     1.399459  0.0     2.0      3.0   \n",
       "neither             24783.0      0.549247     1.113299  0.0     0.0      0.0   \n",
       "class               24783.0      1.110277     0.462089  0.0     1.0      1.0   \n",
       "\n",
       "                        75%      max  \n",
       "Unnamed: 0          18995.5  25296.0  \n",
       "count                   3.0      9.0  \n",
       "hate_speech             0.0      7.0  \n",
       "offensive_language      3.0      9.0  \n",
       "neither                 0.0      9.0  \n",
       "class                   1.0      2.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2916d497",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_transformed = dataset[['class','tweet']]\n",
    "y = dt_transformed.iloc[:, :-1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c177e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7753e1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(transformers=[('encoder',OneHotEncoder(), [0])], remainder='passthrough')\n",
    "y = np.array(ct.fit_transform(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "825918ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " ...\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37090c36",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "I separated this y in two variables that we will use to fit hate speech models and offensive speech models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83d5c2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df = pd.DataFrame(y)\n",
    "y_hate = np.array(y_df[0])\n",
    "y_offensive = np.array(y_df[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bc7bf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 1. 1. ... 1. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_hate)\n",
    "print(y_offensive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fae2641",
   "metadata": {},
   "source": [
    "# Cleaning the texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07b6489d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, 24783):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dt_transformed['tweet'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    ps = PorterStemmer()\n",
    "    all_stopwords = stopwords.words('english')\n",
    "    all_stopwords.remove('not')\n",
    "    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4406e842",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(max_features = 2000)\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bafd910",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1413adad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_hate, test_size = 0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31d3f55",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Finding the best models to predict hate speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7eb0ca",
   "metadata": {},
   "source": [
    "\n",
    "Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0cdf7a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_np = GaussianNB()\n",
    "classifier_np.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da6db86",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1df2065a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(criterion='entropy', random_state=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_dt = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "classifier_dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac060922",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "68520806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "classifier_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195089ff",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Logictic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b8a30d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(random_state=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_lr = LogisticRegression(random_state = 0)\n",
    "classifier_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9f9bcc",
   "metadata": {},
   "source": [
    "\n",
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2eca432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', n_estimators=10, random_state=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\n",
    "classifier_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0df9b8",
   "metadata": {},
   "source": [
    "# Making the Confusion Matrix for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4690575c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting confusion_matrix\n",
      "  Downloading confusion_matrix-0.1.tar.gz (1.9 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting nose\n",
      "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "     ------------------------------------ 154.7/154.7 kB 486.5 kB/s eta 0:00:00Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.0.1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building wheels for collected packages: confusion_matrix\n",
      "  Building wheel for confusion_matrix (setup.py): started\n",
      "  Building wheel for confusion_matrix (setup.py): finished with status 'done'\n",
      "  Created wheel for confusion_matrix: filename=confusion_matrix-0.1-py3-none-any.whl size=2423 sha256=b5e473bf2f361343b6871f0b251f11020b1af659ce68f6236847ad1d006f77a7\n",
      "  Stored in directory: c:\\users\\chandu\\appdata\\local\\pip\\cache\\wheels\\d3\\bc\\0b\\9ef510e823b9e0a8dcb8f8846f66aab7d494e57a2d83068c7e\n",
      "Successfully built confusion_matrix\n",
      "Installing collected packages: nose, confusion_matrix\n",
      "Successfully installed confusion_matrix-0.1 nose-1.3.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c26dacd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\chandu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.0.1)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.1-cp39-cp39-win_amd64.whl (8.4 MB)\n",
      "     ---------------------------------------- 8.4/8.4 MB 456.3 kB/s eta 0:00:00\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     ------------------------------------ 298.0/298.0 kB 497.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\chandu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.21.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\chandu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (1.7.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\chandu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from scikit-learn) (3.0.0)\n",
      "Installing collected packages: joblib, scikit-learn\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.1\n",
      "    Uninstalling scikit-learn-1.0.1:\n",
      "      Successfully uninstalled scikit-learn-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\CHANDU\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python39\\\\Lib\\\\site-packages\\\\~klearn\\\\.libs\\\\vcomp140.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 23.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c5de821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca01323b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1969 2709]\n",
      " [  90  189]]\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "y_pred_np = classifier_np.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred_np)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b4ab6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4455  223]\n",
      " [ 188   91]]\n"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "y_pred_dt = classifier_dt.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred_dt)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "350050f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4614   64]\n",
      " [ 229   50]]\n"
     ]
    }
   ],
   "source": [
    "#Linear Regression\n",
    "y_pred_lr = classifier_lr.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "93e62db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4586   92]\n",
      " [ 213   66]]\n"
     ]
    }
   ],
   "source": [
    "#Random Florest\n",
    "y_pred_rf = classifier_rf.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1e2aa4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2663262b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy:  0.9384708493040145\n",
      "Linear Regression Accuracy:  0.940891668347791\n",
      "Decision Tree Accuracy:  0.9170869477506557\n",
      "Naive Bayes Accuracy:  0.43534395803913656\n"
     ]
    }
   ],
   "source": [
    "rf_score = accuracy_score(y_test, y_pred_rf)\n",
    "lr_score = accuracy_score(y_test, y_pred_lr)\n",
    "dt_score = accuracy_score(y_test, y_pred_dt)\n",
    "np_score = accuracy_score(y_test, y_pred_np)\n",
    "\n",
    "print('Random Forest Accuracy: ', str(rf_score))\n",
    "print('Linear Regression Accuracy: ', str(lr_score))\n",
    "print('Decision Tree Accuracy: ', str(dt_score))\n",
    "print('Naive Bayes Accuracy: ', str(np_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ab55ce",
   "metadata": {},
   "source": [
    "So, Linear Regression looks better to predict hate speech based on this dataset. It's important to emphasize Random Forest and Decision Tree had great results as well. This Dataset looks like a product of artificial intelligence to classify hate speech and offensive terms. What I think way more technological than these models that I implemented here. It was just a study of the implementation of NLP based on the bag of words method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68d5ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
